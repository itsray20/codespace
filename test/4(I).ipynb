{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /usr/local/python/3.10.8/lib/python3.10/site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from gensim) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /home/codespace/.local/lib/python3.10/site-packages (from gensim) (1.11.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from gensim) (6.4.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/python/3.10.8/lib/python3.10/site-packages (4.33.2)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.10/site-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from transformers) (0.17.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.10/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec in /usr/local/python/3.10.8/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.9.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /usr/local/python/3.10.8/lib/python3.10/site-packages (0.1.99)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting newspaper3k\n",
      "  Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.4.1 in /home/codespace/.local/lib/python3.10/site-packages (from newspaper3k) (4.12.2)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in /home/codespace/.local/lib/python3.10/site-packages (from newspaper3k) (10.0.0)\n",
      "Requirement already satisfied: PyYAML>=3.11 in /home/codespace/.local/lib/python3.10/site-packages (from newspaper3k) (6.0.1)\n",
      "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
      "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: lxml>=3.6.0 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from newspaper3k) (4.9.3)\n",
      "Requirement already satisfied: nltk>=3.2.1 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from newspaper3k) (3.8.1)\n",
      "Requirement already satisfied: requests>=2.10.0 in /home/codespace/.local/lib/python3.10/site-packages (from newspaper3k) (2.31.0)\n",
      "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
      "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tldextract>=2.0.1 (from newspaper3k)\n",
      "  Obtaining dependency information for tldextract>=2.0.1 from https://files.pythonhosted.org/packages/e4/6b/2e0c1449c0768f25ea8054476a991152a59507ac019a5647d92e44540a73/tldextract-3.5.0-py3-none-any.whl.metadata\n",
      "  Downloading tldextract-3.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
      "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting jieba3k>=0.35.1 (from newspaper3k)\n",
      "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.5.3 in /home/codespace/.local/lib/python3.10/site-packages (from newspaper3k) (2.8.2)\n",
      "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
      "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /home/codespace/.local/lib/python3.10/site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.5)\n",
      "Requirement already satisfied: six in /home/codespace/.local/lib/python3.10/site-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
      "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: click in /usr/local/python/3.10.8/lib/python3.10/site-packages (from nltk>=3.2.1->newspaper3k) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/codespace/.local/lib/python3.10/site-packages (from nltk>=3.2.1->newspaper3k) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from nltk>=3.2.1->newspaper3k) (2023.8.8)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.10.8/lib/python3.10/site-packages (from nltk>=3.2.1->newspaper3k) (4.66.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.10.0->newspaper3k) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.10.0->newspaper3k) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from requests>=2.10.0->newspaper3k) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.10.0->newspaper3k) (2023.7.22)\n",
      "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
      "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
      "Requirement already satisfied: filelock>=3.0.8 in /home/codespace/.local/lib/python3.10/site-packages (from tldextract>=2.0.1->newspaper3k) (3.12.4)\n",
      "Downloading tldextract-3.5.0-py3-none-any.whl (97 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.2/97.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
      "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13541 sha256=236b66b4b3964b8a0f715f6feb865ddf35230baddd3f08a9fdff691a81e79578\n",
      "  Stored in directory: /home/codespace/.cache/pip/wheels/c8/d6/6c/384f58df48c00b9a31d638005143b5b3ac62c3d25fb1447f23\n",
      "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3339 sha256=9e41fcb6c487cc3b07d46c588c0e12ff456fc75cb78cd56543c11e4030f809ad\n",
      "  Stored in directory: /home/codespace/.cache/pip/wheels/97/02/e7/a1ff1760e12bdbaab0ac824fae5c1bc933e41c4ccd6a8f8edb\n",
      "  Building wheel for jieba3k (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398380 sha256=8237f72deb7b12dbc7e34f884a83c218c23014c5f157fa31f21b8d343982505c\n",
      "  Stored in directory: /home/codespace/.cache/pip/wheels/7a/c4/0c/12a9a314ecac499456c4c3b2fcc2f635a3b45a39dfbd240299\n",
      "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=d0962371feede2b671d2f63468b9d34358648f69569bd06f915d826d057dc973\n",
      "  Stored in directory: /home/codespace/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
      "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
      "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, feedparser, cssselect, requests-file, feedfinder2, tldextract, newspaper3k\n",
      "Successfully installed cssselect-1.2.0 feedfinder2-0.0.4 feedparser-6.0.10 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.5.1 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF report generated as 'website_analysis_report.pdf'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from newspaper import Article\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image, Table, ListFlowable, ListItem\n",
    "from reportlab.lib.styles import getSampleStyleSheet\n",
    "from reportlab.lib import colors\n",
    "from io import BytesIO\n",
    "\n",
    "# Function to fetch content from a URL using Newspaper3k\n",
    "def fetch_content(url):\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        return article.title, article.text\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while fetching content: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "# Function to extract and preprocess content from HTML\n",
    "def extract_and_preprocess_content(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    title = soup.title.string if soup.title else \"Title not found\"\n",
    "    paragraphs = soup.find_all('p')\n",
    "    content = ' '.join([p.get_text() for p in paragraphs])\n",
    "    return title, content\n",
    "\n",
    "# Function to preprocess text, tokenize, and create a frequency table\n",
    "def preprocess_text(text):\n",
    "    # Tokenize text into words\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Initialize a stop words set\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    words = [word.lower() for word in words if word.isalpha()]\n",
    "\n",
    "    # Remove stopwords\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Create a frequency distribution\n",
    "    fdist = FreqDist(words)\n",
    "\n",
    "    return fdist\n",
    "\n",
    "# Function to extract pointwise website summary\n",
    "def extract_pointwise_summary(sentences, frequency_table, num_sentences=5):\n",
    "    # Sort sentences by their importance based on word frequency\n",
    "    ranked_sentences = sorted(sentences, key=lambda x: sum(frequency_table[word] for word in word_tokenize(x.lower())), reverse=True)\n",
    "\n",
    "    # Select the top N sentences as the summary\n",
    "    summary = ranked_sentences[:num_sentences]\n",
    "\n",
    "    # Detokenize the selected sentences to form the summary\n",
    "    summary_text = TreebankWordDetokenizer().detokenize(summary)\n",
    "\n",
    "    return summary_text\n",
    "\n",
    "# Function to generate a PDF report\n",
    "def generate_pdf_report(website_url, title, content, keyword_frequency, website_summary):\n",
    "    pdf_buffer = BytesIO()\n",
    "    doc = SimpleDocTemplate(pdf_buffer, pagesize=letter)\n",
    "\n",
    "    elements = []\n",
    "\n",
    "    # Title\n",
    "    title_text = f\"Website Analysis Report for '{title}'\"\n",
    "    title_paragraph = Paragraph(title_text, getSampleStyleSheet()['Title'])\n",
    "    elements.append(title_paragraph)\n",
    "    elements.append(Spacer(1, 12))\n",
    "\n",
    "    # URL\n",
    "    url_paragraph = Paragraph(f\"URL: {website_url}\", getSampleStyleSheet()['Normal'])\n",
    "    elements.append(url_paragraph)\n",
    "    elements.append(Spacer(1, 12))\n",
    "\n",
    "    # Important Details\n",
    "    details_paragraph = Paragraph(\"Important Details:\", getSampleStyleSheet()['Heading2'])\n",
    "    elements.append(details_paragraph)\n",
    "    elements.append(Spacer(1, 6))\n",
    "\n",
    "    details_text = f\"Title: {title}\\n\"\n",
    "    details_paragraph = Paragraph(details_text, getSampleStyleSheet()['Normal'])\n",
    "    elements.append(details_paragraph)\n",
    "    elements.append(Spacer(1, 12))\n",
    "\n",
    "    # Website Summary (Pointwise)\n",
    "    summary_paragraph = Paragraph(\"Website Summary:\", getSampleStyleSheet()['Heading2'])\n",
    "    elements.append(summary_paragraph)\n",
    "    elements.append(Spacer(1, 6))\n",
    "\n",
    "    # Generate a pointwise summary\n",
    "    pointwise_summary = extract_pointwise_summary(sent_tokenize(content), keyword_frequency, num_sentences=5)\n",
    "    summary_paragraph = Paragraph(pointwise_summary, getSampleStyleSheet()['Normal'])\n",
    "    elements.append(summary_paragraph)\n",
    "    elements.append(Spacer(1, 12))\n",
    "\n",
    "    # Content Analysis\n",
    "    content_analysis = Paragraph(\"Content Analysis:\", getSampleStyleSheet()['Heading2'])\n",
    "    elements.append(content_analysis)\n",
    "    elements.append(Spacer(1, 6))\n",
    "\n",
    "    # Keywords Frequency Table (simplified)\n",
    "    keyword_table_data = []\n",
    "    for word, freq in keyword_frequency.items():\n",
    "        keyword_table_data.append([word, freq])\n",
    "\n",
    "    keyword_table_style = [\n",
    "        ('GRID', (0, 0), (-1, -1), 1, colors.black),\n",
    "        ('ALIGN', (1, 1), (-1, -1), 'RIGHT'),\n",
    "    ]\n",
    "    keyword_table = Table(keyword_table_data, colWidths=[200, 100], style=keyword_table_style)\n",
    "    elements.append(keyword_table)\n",
    "    elements.append(Spacer(1, 12))\n",
    "\n",
    "    # Save PDF\n",
    "    doc.build(elements)\n",
    "\n",
    "    # Save the PDF to a file\n",
    "    pdf_filename = 'website_analysis_report.pdf'\n",
    "    with open(pdf_filename, 'wb') as pdf_file:\n",
    "        pdf_file.write(pdf_buffer.getvalue())\n",
    "\n",
    "    print(f\"PDF report generated as '{pdf_filename}'.\")\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    website_url = 'https://codewithcurious.com/projects/chatgpt-using-python/'  # Replace with the target website URL\n",
    "    title, content = fetch_content(website_url)\n",
    "\n",
    "    if content:\n",
    "        # Preprocess text and create a frequency table\n",
    "        keyword_frequency = preprocess_text(content)\n",
    "\n",
    "        # Generate a pointwise website summary\n",
    "        website_summary = extract_pointwise_summary(sent_tokenize(content), keyword_frequency, num_sentences=5)\n",
    "\n",
    "        # Generate PDF report\n",
    "        generate_pdf_report(website_url, title, content, keyword_frequency, website_summary)\n",
    "    else:\n",
    "        print(\"Failed to fetch content from the URL.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
