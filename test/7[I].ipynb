{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /usr/local/python/3.10.8/lib/python3.10/site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from gensim) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /home/codespace/.local/lib/python3.10/site-packages (from gensim) (1.11.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from gensim) (6.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/python/3.10.8/lib/python3.10/site-packages (4.33.2)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.10/site-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from transformers) (0.17.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.10/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec in /usr/local/python/3.10.8/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.9.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /usr/local/python/3.10.8/lib/python3.10/site-packages (0.1.99)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: newspaper3k in /usr/local/python/3.10.8/lib/python3.10/site-packages (0.2.8)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in /home/codespace/.local/lib/python3.10/site-packages (from newspaper3k) (4.12.2)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in /home/codespace/.local/lib/python3.10/site-packages (from newspaper3k) (10.0.0)\n",
      "Requirement already satisfied: PyYAML>=3.11 in /home/codespace/.local/lib/python3.10/site-packages (from newspaper3k) (6.0.1)\n",
      "Requirement already satisfied: cssselect>=0.9.2 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from newspaper3k) (1.2.0)\n",
      "Requirement already satisfied: lxml>=3.6.0 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from newspaper3k) (4.9.3)\n",
      "Requirement already satisfied: nltk>=3.2.1 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from newspaper3k) (3.8.1)\n",
      "Requirement already satisfied: requests>=2.10.0 in /home/codespace/.local/lib/python3.10/site-packages (from newspaper3k) (2.31.0)\n",
      "Requirement already satisfied: feedparser>=5.2.1 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from newspaper3k) (6.0.10)\n",
      "Requirement already satisfied: tldextract>=2.0.1 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from newspaper3k) (3.5.0)\n",
      "Requirement already satisfied: feedfinder2>=0.0.4 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from newspaper3k) (0.0.4)\n",
      "Requirement already satisfied: jieba3k>=0.35.1 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from newspaper3k) (0.35.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /home/codespace/.local/lib/python3.10/site-packages (from newspaper3k) (2.8.2)\n",
      "Requirement already satisfied: tinysegmenter==0.3 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from newspaper3k) (0.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/codespace/.local/lib/python3.10/site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.5)\n",
      "Requirement already satisfied: six in /home/codespace/.local/lib/python3.10/site-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
      "Requirement already satisfied: sgmllib3k in /usr/local/python/3.10.8/lib/python3.10/site-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
      "Requirement already satisfied: click in /usr/local/python/3.10.8/lib/python3.10/site-packages (from nltk>=3.2.1->newspaper3k) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/codespace/.local/lib/python3.10/site-packages (from nltk>=3.2.1->newspaper3k) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from nltk>=3.2.1->newspaper3k) (2023.8.8)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.10.8/lib/python3.10/site-packages (from nltk>=3.2.1->newspaper3k) (4.66.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.10.0->newspaper3k) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.10.0->newspaper3k) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from requests>=2.10.0->newspaper3k) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.10.0->newspaper3k) (2023.7.22)\n",
      "Requirement already satisfied: requests-file>=1.4 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from tldextract>=2.0.1->newspaper3k) (1.5.1)\n",
      "Requirement already satisfied: filelock>=3.0.8 in /home/codespace/.local/lib/python3.10/site-packages (from tldextract>=2.0.1->newspaper3k) (3.12.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rake-nltk in /usr/local/python/3.10.8/lib/python3.10/site-packages (1.0.6)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.6.2 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from rake-nltk) (3.8.1)\n",
      "Requirement already satisfied: click in /usr/local/python/3.10.8/lib/python3.10/site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/codespace/.local/lib/python3.10/site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (2023.8.8)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.10.8/lib/python3.10/site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (4.66.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install rake-nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /usr/local/python/3.10.8/lib/python3.10/site-packages (0.28.0)\n",
      "Requirement already satisfied: requests>=2.20 in /home/codespace/.local/lib/python3.10/site-packages (from openai) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.10.8/lib/python3.10/site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/python/3.10.8/lib/python3.10/site-packages (from openai) (3.8.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.20->openai) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from requests>=2.20->openai) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.20->openai) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/codespace/.local/lib/python3.10/site-packages (from aiohttp->openai) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from aiohttp->openai) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from aiohttp->openai) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from aiohttp->openai) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yake\n",
      "  Downloading yake-0.4.8-py2.py3-none-any.whl (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.2/60.2 kB\u001b[0m \u001b[31m700.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/python/3.10.8/lib/python3.10/site-packages (from yake) (0.9.0)\n",
      "Requirement already satisfied: click>=6.0 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from yake) (8.1.7)\n",
      "Requirement already satisfied: numpy in /usr/local/python/3.10.8/lib/python3.10/site-packages (from yake) (1.24.3)\n",
      "Collecting segtok (from yake)\n",
      "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.10/site-packages (from yake) (3.1)\n",
      "Collecting jellyfish (from yake)\n",
      "  Obtaining dependency information for jellyfish from https://files.pythonhosted.org/packages/ac/5a/fafb2fe555f34e5aeed8c11153257c5af09197451eecb36207e4e2973aed/jellyfish-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading jellyfish-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: regex in /usr/local/python/3.10.8/lib/python3.10/site-packages (from segtok->yake) (2023.8.8)\n",
      "Downloading jellyfish-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: segtok, jellyfish, yake\n",
      "Successfully installed jellyfish-1.0.0 segtok-1.5.11 yake-0.4.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install yake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF report generated as 'website_analysis_report.pdf'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from newspaper import Article\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image, Table, ListFlowable, ListItem\n",
    "from reportlab.lib.styles import getSampleStyleSheet\n",
    "from reportlab.lib import colors\n",
    "from io import BytesIO\n",
    "from yake import KeywordExtractor  # Import YAKE for keyword extraction\n",
    "\n",
    "# Function to fetch content from a URL using Newspaper3k\n",
    "def fetch_content(url):\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        return article.title, article.text\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while fetching content: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "# Function to extract and preprocess content from HTML\n",
    "def extract_and_preprocess_content(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    title = soup.title.string if soup.title else \"Title not found\"\n",
    "    paragraphs = soup.find_all('p')\n",
    "    content = ' '.join([p.get_text() for p in paragraphs])\n",
    "    return title, content\n",
    "\n",
    "# Function to preprocess text, tokenize, and create a frequency table\n",
    "def preprocess_text(text):\n",
    "    # Tokenize text into words\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Initialize a stop words set\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    words = [word.lower() for word in words if word.isalpha()]\n",
    "\n",
    "    # Remove stopwords\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Create a frequency distribution\n",
    "    fdist = FreqDist(words)\n",
    "\n",
    "    return fdist\n",
    "\n",
    "# Function to extract pointwise website summary\n",
    "def extract_pointwise_summary(sentences, frequency_table, num_sentences=5):\n",
    "    # Sort sentences by their importance based on word frequency\n",
    "    ranked_sentences = sorted(sentences, key=lambda x: sum(frequency_table[word] for word in word_tokenize(x.lower())), reverse=True)\n",
    "\n",
    "    # Select the top N sentences as the summary\n",
    "    summary = ranked_sentences[:num_sentences]\n",
    "\n",
    "    # Detokenize the selected sentences to form the summary\n",
    "    summary_text = TreebankWordDetokenizer().detokenize(summary)\n",
    "\n",
    "    return summary_text\n",
    "\n",
    "# Function to count images, URLs, and videos on the website\n",
    "def count_website_media(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            images = len(soup.find_all('img'))\n",
    "            links = len(soup.find_all('a', href=True))\n",
    "            videos = len(soup.find_all('video'))\n",
    "            return images, links, videos\n",
    "        else:\n",
    "            print(f\"Failed to fetch content from {url}. Status code: {response.status_code}\")\n",
    "            return None, None, None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while counting media: {str(e)}\")\n",
    "        return None, None, None\n",
    "\n",
    "# Function to dynamically generate information about the website using YAKE for keyword extraction\n",
    "def generate_website_info(content):\n",
    "    # Initialize the YAKE keyword extractor\n",
    "    keyword_extractor = KeywordExtractor()\n",
    "\n",
    "    # Extract keywords from the content\n",
    "    keywords = keyword_extractor.extract_keywords(content)\n",
    "\n",
    "    # Extract the top keywords and create a description\n",
    "    top_keywords = [keyword for keyword, score in keywords[:5]]  # Adjust the number of keywords as needed\n",
    "    website_info = f\"This website is related to {', '.join(top_keywords)}.\"\n",
    "\n",
    "    return website_info\n",
    "\n",
    "# Function to generate a PDF report\n",
    "def generate_pdf_report(website_url, title, content, keyword_frequency, website_summary, images_count, links_count, videos_count, website_info):\n",
    "    pdf_buffer = BytesIO()\n",
    "    doc = SimpleDocTemplate(pdf_buffer, pagesize=letter)\n",
    "\n",
    "    elements = []\n",
    "\n",
    "    # Title\n",
    "    title_text = f\"Website Analysis Report for '{title}'\"\n",
    "    title_paragraph = Paragraph(title_text, getSampleStyleSheet()['Title'])\n",
    "    elements.append(title_paragraph)\n",
    "    elements.append(Spacer(1, 12))\n",
    "\n",
    "    # URL\n",
    "    url_paragraph = Paragraph(f\"URL: {website_url}\", getSampleStyleSheet()['Normal'])\n",
    "    elements.append(url_paragraph)\n",
    "    elements.append(Spacer(1, 12))\n",
    "\n",
    "    # Important Details\n",
    "    details_paragraph = Paragraph(\"Important Details:\", getSampleStyleSheet()['Heading2'])\n",
    "    elements.append(details_paragraph)\n",
    "    elements.append(Spacer(1, 6))\n",
    "\n",
    "    details_text = f\"Title: {title}\\n\"\n",
    "    details_paragraph = Paragraph(details_text, getSampleStyleSheet()['Normal'])\n",
    "    elements.append(details_paragraph)\n",
    "    elements.append(Spacer(1, 12))\n",
    "\n",
    "    # About the Website (Dynamic Info)\n",
    "    about_website_paragraph = Paragraph(\"About the Website:\", getSampleStyleSheet()['Heading2'])\n",
    "    elements.append(about_website_paragraph)\n",
    "    elements.append(Spacer(1, 6))\n",
    "\n",
    "    # Include the dynamically generated website information\n",
    "    about_website_paragraph = Paragraph(website_info, getSampleStyleSheet()['Normal'])\n",
    "    elements.append(about_website_paragraph)\n",
    "    elements.append(Spacer(1, 12))\n",
    "\n",
    "    # Keywords Section\n",
    "    keywords_paragraph = Paragraph(\"Keywords:\", getSampleStyleSheet()['Heading2'])\n",
    "    elements.append(keywords_paragraph)\n",
    "    elements.append(Spacer(1, 6))\n",
    "\n",
    "    # Display the highly important keywords (you can customize the number)\n",
    "    num_keywords_to_display = 10\n",
    "    important_keywords = keyword_frequency.most_common(num_keywords_to_display)\n",
    "    keywords_text = \", \".join([f\"{word} ({freq} times)\" for word, freq in important_keywords])\n",
    "    keywords_paragraph = Paragraph(keywords_text, getSampleStyleSheet()['Normal'])\n",
    "    elements.append(keywords_paragraph)\n",
    "    elements.append(Spacer(1, 12))\n",
    "\n",
    "    # Media Information\n",
    "    media_paragraph = Paragraph(\"Media Information:\", getSampleStyleSheet()['Heading2'])\n",
    "    elements.append(media_paragraph)\n",
    "    elements.append(Spacer(1, 6))\n",
    "\n",
    "    media_text = f\"Images: {images_count}\\nLinks: {links_count}\\nVideos: {videos_count}\"\n",
    "    media_paragraph = Paragraph(media_text, getSampleStyleSheet()['Normal'])\n",
    "    elements.append(media_paragraph)\n",
    "    elements.append(Spacer(1, 12))\n",
    "\n",
    "    # Website Summary (Pointwise)\n",
    "    summary_paragraph = Paragraph(\"Website Summary:\", getSampleStyleSheet()['Heading2'])\n",
    "    elements.append(summary_paragraph)\n",
    "    elements.append(Spacer(1, 6))\n",
    "\n",
    "    # Generate a pointwise summary\n",
    "    summary_paragraph = Paragraph(website_summary, getSampleStyleSheet()['Normal'])\n",
    "    elements.append(summary_paragraph)\n",
    "    elements.append(Spacer(1, 12))\n",
    "\n",
    "    # Save PDF\n",
    "    doc.build(elements)\n",
    "\n",
    "    # Save the PDF to a file\n",
    "    pdf_filename = 'website_analysis_report.pdf'\n",
    "    with open(pdf_filename, 'wb') as pdf_file:\n",
    "        pdf_file.write(pdf_buffer.getvalue())\n",
    "\n",
    "    print(f\"PDF report generated as '{pdf_filename}'.\")\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    website_url = 'https://www.sbicard.com/'  # Replace with the target website URL\n",
    "    title, content = fetch_content(website_url)\n",
    "\n",
    "    if content:\n",
    "        # Preprocess text and create a frequency table\n",
    "        keyword_frequency = preprocess_text(content)\n",
    "\n",
    "        # Count images, URLs, and videos on the website\n",
    "        images_count, links_count, videos_count = count_website_media(website_url)\n",
    "\n",
    "        # Generate a pointwise website summary\n",
    "        website_summary = extract_pointwise_summary(sent_tokenize(content), keyword_frequency, num_sentences=5)\n",
    "\n",
    "        # Generate dynamic website information\n",
    "        website_info = generate_website_info(content)\n",
    "\n",
    "        # Generate PDF report\n",
    "        generate_pdf_report(website_url, title, content, keyword_frequency, website_summary, images_count, links_count, videos_count, website_info)\n",
    "    else:\n",
    "        print(\"Failed to fetch content from the URL.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
