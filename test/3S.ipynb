{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /usr/local/python/3.10.8/lib/python3.10/site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from gensim) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /home/codespace/.local/lib/python3.10/site-packages (from gensim) (1.11.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from gensim) (6.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/python/3.10.8/lib/python3.10/site-packages (4.33.2)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.10/site-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from transformers) (0.17.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.10/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec in /usr/local/python/3.10.8/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.9.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /usr/local/python/3.10.8/lib/python3.10/site-packages (0.1.99)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF report generated as 'website_analysis_report.pdf'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image, Table, ListFlowable, ListItem\n",
    "from reportlab.lib.styles import getSampleStyleSheet\n",
    "from reportlab.lib import colors\n",
    "from io import BytesIO\n",
    "\n",
    "# Function to fetch content from a URL\n",
    "def fetch_content(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            return response.text\n",
    "        else:\n",
    "            print(f\"Failed to fetch content from {url}. Status code: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while fetching content: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Function to extract and preprocess content from HTML\n",
    "def extract_and_preprocess_content(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    title = soup.title.string if soup.title else \"Title not found\"\n",
    "    paragraphs = soup.find_all('p')\n",
    "    content = ' '.join([p.get_text() for p in paragraphs])\n",
    "    return title, content\n",
    "\n",
    "# Function to preprocess text, tokenize, and create a frequency table\n",
    "def preprocess_text(text):\n",
    "    # Tokenize text into words\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Initialize a stop words set\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    words = [word.lower() for word in words if word.isalpha()]\n",
    "\n",
    "    # Remove stopwords\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Create a frequency distribution\n",
    "    fdist = FreqDist(words)\n",
    "\n",
    "    return fdist\n",
    "\n",
    "# Function to extract pointwise website summary\n",
    "def extract_pointwise_summary(sentences, frequency_table, num_sentences=5):\n",
    "    # Sort sentences by their importance based on word frequency\n",
    "    ranked_sentences = sorted(sentences, key=lambda x: sum(frequency_table[word] for word in word_tokenize(x.lower())), reverse=True)\n",
    "\n",
    "    # Select the top N sentences as the summary\n",
    "    summary = ranked_sentences[:num_sentences]\n",
    "\n",
    "    # Detokenize the selected sentences to form the summary\n",
    "    summary_text = TreebankWordDetokenizer().detokenize(summary)\n",
    "\n",
    "    return summary_text\n",
    "\n",
    "# Function to generate a PDF report\n",
    "def generate_pdf_report(website_url, title, content, keyword_frequency, website_summary):\n",
    "    pdf_buffer = BytesIO()\n",
    "    doc = SimpleDocTemplate(pdf_buffer, pagesize=letter)\n",
    "\n",
    "    elements = []\n",
    "\n",
    "    # Title\n",
    "    title_text = f\"Website Analysis Report for '{title}'\"\n",
    "    title_paragraph = Paragraph(title_text, getSampleStyleSheet()['Title'])\n",
    "    elements.append(title_paragraph)\n",
    "    elements.append(Spacer(1, 12))\n",
    "\n",
    "    # URL\n",
    "    url_paragraph = Paragraph(f\"URL: {website_url}\", getSampleStyleSheet()['Normal'])\n",
    "    elements.append(url_paragraph)\n",
    "    elements.append(Spacer(1, 12))\n",
    "\n",
    "    # Important Details\n",
    "    details_paragraph = Paragraph(\"Important Details:\", getSampleStyleSheet()['Heading2'])\n",
    "    elements.append(details_paragraph)\n",
    "    elements.append(Spacer(1, 6))\n",
    "\n",
    "    details_text = f\"Title: {title}\\n\"\n",
    "    details_paragraph = Paragraph(details_text, getSampleStyleSheet()['Normal'])\n",
    "    elements.append(details_paragraph)\n",
    "    elements.append(Spacer(1, 12))\n",
    "\n",
    "    # Website Summary (Pointwise)\n",
    "    summary_paragraph = Paragraph(\"Website Summary:\", getSampleStyleSheet()['Heading2'])\n",
    "    elements.append(summary_paragraph)\n",
    "    elements.append(Spacer(1, 6))\n",
    "\n",
    "    # Generate a pointwise summary\n",
    "    pointwise_summary = extract_pointwise_summary(sent_tokenize(content), keyword_frequency, num_sentences=5)\n",
    "    summary_paragraph = Paragraph(pointwise_summary, getSampleStyleSheet()['Normal'])\n",
    "    elements.append(summary_paragraph)\n",
    "    elements.append(Spacer(1, 12))\n",
    "\n",
    "    # Content Analysis\n",
    "    content_analysis = Paragraph(\"Content Analysis:\", getSampleStyleSheet()['Heading2'])\n",
    "    elements.append(content_analysis)\n",
    "    elements.append(Spacer(1, 6))\n",
    "\n",
    "    # Keywords Frequency Table (simplified)\n",
    "    keyword_table_data = []\n",
    "    for word, freq in keyword_frequency.items():\n",
    "        keyword_table_data.append([word, freq])\n",
    "\n",
    "    keyword_table_style = [\n",
    "        ('GRID', (0, 0), (-1, -1), 1, colors.black),\n",
    "        ('ALIGN', (1, 1), (-1, -1), 'RIGHT'),\n",
    "    ]\n",
    "    keyword_table = Table(keyword_table_data, colWidths=[200, 100], style=keyword_table_style)\n",
    "    elements.append(keyword_table)\n",
    "    elements.append(Spacer(1, 12))\n",
    "\n",
    "    # Save PDF\n",
    "    doc.build(elements)\n",
    "\n",
    "    # Save the PDF to a file\n",
    "    pdf_filename = 'website_analysis_report.pdf'\n",
    "    with open(pdf_filename, 'wb') as pdf_file:\n",
    "        pdf_file.write(pdf_buffer.getvalue())\n",
    "\n",
    "    print(f\"PDF report generated as '{pdf_filename}'.\")\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    website_url = 'https://medium.com/dataflair/these-projects-will-make-you-the-superhero-of-python-city-14101e62393b'  # Replace with the target website URL\n",
    "    content = fetch_content(website_url)\n",
    "\n",
    "    if content:\n",
    "        # Extract and preprocess content\n",
    "        title, article_content = extract_and_preprocess_content(content)\n",
    "\n",
    "        # Preprocess text and create a frequency table\n",
    "        keyword_frequency = preprocess_text(article_content)\n",
    "\n",
    "        # Generate a pointwise website summary\n",
    "        website_summary = extract_pointwise_summary(sent_tokenize(article_content), keyword_frequency, num_sentences=5)\n",
    "\n",
    "        # Generate PDF report\n",
    "        generate_pdf_report(website_url, title, article_content, keyword_frequency, website_summary)\n",
    "    else:\n",
    "        print(\"Failed to fetch content from the URL.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
